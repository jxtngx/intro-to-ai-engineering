{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMSrsl5zk8KBvgGA1IlId4u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jxtngx/llamastack-cookbook/blob/main/python/getting_started/L0_Exploring_Llama_Stack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Gentle Intro to LLama Stack"
      ],
      "metadata": {
        "id": "GPGzaVB43bMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step1: install llama-stack"
      ],
      "metadata": {
        "id": "02ul6mkwVwul"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtX3glbdBUU5",
        "outputId": "89a2f819-b5e5-47b8-9136-9c5040b735f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.2/16.2 MB 83.6 MB/s eta 0:00:00\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install uv -q\n",
        "uv pip install llama-stack -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: build llama-stack"
      ],
      "metadata": {
        "id": "Cq0OnnOcV4Ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "llama stack build --template meta-reference-gpu --image-type venv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUQqenT_BeIp",
        "outputId": "fe57797c-f6c9-4480-a988-9d1df7a9e20d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Script Output\n",
            " Installing dependencies in system Python environment\n",
            "Installing pip dependencies\n",
            "\n",
            "Build Successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: print the list of models"
      ],
      "metadata": {
        "id": "X7vnieVqV72V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "llama model list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dCEc9sZD6xK",
        "outputId": "d9ff5b25-5e30-448e-c9c7-40225d299af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n",
            "┃ Model Descriptor(ID)                   ┃ Hugging Face Repo                      ┃ Context Length ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n",
            "│ Llama3.1-8B                            │ meta-llama/Llama-3.1-8B                │ 128K           │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama3.1-70B                           │ meta-llama/Llama-3.1-70B               │ 128K           │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama3.1-405B:bf16-mp8                 │ meta-llama/Llama-3.1-405B              │ 128K           │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama3.1-405B                          │ meta-llama/Llama-3.1-405B-FP8          │ 128K           │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama3.1-405B:bf16-mp16                │ meta-llama/Llama-3.1-405B              │ 128K           │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama3.1-8B-Instruct                   │ meta-llama/Llama-3.1-8B-Instruct       │ 128K           │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama3.1-70B-Instruct                  │ meta-llama/Llama-3.1-70B-Instruct      │ 128K           │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama3.1-405B-Instruct:bf16-mp8        │ meta-llama/Llama-3.1-405B-Instruct     │ 128K           │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama3.1-405B-Instruct                 │ meta-llama/Llama-3.1-405B-Instruct-FP8 │ 128K           │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama3.1-405B-Instruct:bf16-mp16       │ meta-llama/Llama-3.1-405B-Instruct     │ 128K           │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama3.2-1B                            │ meta-llama/Llama-3.2-1B                │ 128K           │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama3.2-3B                            │ meta-llama/Llama-3.2-3B                │ 128K           │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama3.2-11B-Vision                    │ meta-llama/Llama-3.2-11B-Vision        │ 128K           │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama3.2-90B-Vision                    │ meta-llama/Llama-3.2-90B-Vision        │ 128K           │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama3.2-1B-Instruct                   │ meta-llama/Llama-3.2-1B-Instruct       │ 128K           │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama3.2-3B-Instruct                   │ meta-llama/Llama-3.2-3B-Instruct       │ 128K           │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama3.2-1B-Instruct:int4-qlora-eo8    │ meta-llama/Llama-3.2-1B-Instruct-QLOR… │ 8K             │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama3.2-1B-Instruct:int4-spinquant-e… │ meta-llama/Llama-3.2-1B-Instruct-Spin… │ 8K             │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama3.2-3B-Instruct:int4-qlora-eo8    │ meta-llama/Llama-3.2-3B-Instruct-QLOR… │ 8K             │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama3.2-3B-Instruct:int4-spinquant-e… │ meta-llama/Llama-3.2-3B-Instruct-Spin… │ 8K             │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama3.2-11B-Vision-Instruct           │ meta-llama/Llama-3.2-11B-Vision-Instr… │ 128K           │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama3.2-90B-Vision-Instruct           │ meta-llama/Llama-3.2-90B-Vision-Instr… │ 128K           │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama3.3-70B-Instruct                  │ meta-llama/Llama-3.3-70B-Instruct      │ 128K           │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama-Guard-3-11B-Vision               │ meta-llama/Llama-Guard-3-11B-Vision    │ 128K           │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama-Guard-3-1B:int4                  │ meta-llama/Llama-Guard-3-1B-INT4       │ 128K           │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama-Guard-3-1B                       │ meta-llama/Llama-Guard-3-1B            │ 128K           │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama-Guard-3-8B                       │ meta-llama/Llama-Guard-3-8B            │ 128K           │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama-Guard-3-8B:int8                  │ meta-llama/Llama-Guard-3-8B-INT8       │ 128K           │\n",
            "├────────────────────────────────────────┼────────────────────────────────────────┼────────────────┤\n",
            "│ Llama-Guard-2-8B                       │ meta-llama/Llama-Guard-2-8B            │ 4K             │\n",
            "└────────────────────────────────────────┴────────────────────────────────────────┴────────────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: explore the llama CLI"
      ],
      "metadata": {
        "id": "IYhCCopwWB70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "llama model --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWDhM6l9EAG0",
        "outputId": "c31678f9-4dfe-4c31-8249-b524315fd60c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: llama model [-h] {download,list,prompt-format,describe,verify-download,remove} ...\n",
            "\n",
            "Work with llama models\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "\n",
            "model_subcommands:\n",
            "  {download,list,prompt-format,describe,verify-download,remove}\n",
            "\n",
            "  download              Download a model from llama.meta.com or Hugging Face Hub\n",
            "  list                  Show available llama models\n",
            "  prompt-format         Show llama model message formats\n",
            "  describe              Show details about a llama model\n",
            "  verify-download       Verify the downloaded checkpoints' checksums for models downloaded from Meta\n",
            "  remove                Remove the downloaded llama model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "llama model download --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mq2flj5_EJnH",
        "outputId": "b7af8582-9105-4836-da68-533cf764df40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: llama model download [-h] [--source {meta,huggingface}] [--model-id MODEL_ID]\n",
            "                            [--hf-token HF_TOKEN] [--meta-url META_URL]\n",
            "                            [--max-parallel MAX_PARALLEL] [--ignore-patterns IGNORE_PATTERNS]\n",
            "                            [--manifest-file MANIFEST_FILE]\n",
            "\n",
            "Download a model from llama.meta.com or Hugging Face Hub\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --source {meta,huggingface}\n",
            "  --model-id MODEL_ID   See `llama model list` or `llama model list --show-all` for the list of available models. Specify multiple model IDs with commas, e.g. --model-id Llama3.2-1B,Llama3.2-3B\n",
            "  --hf-token HF_TOKEN   Hugging Face API token. Needed for gated models like llama2/3. Will also try to read environment variable `HF_TOKEN` as default.\n",
            "  --meta-url META_URL   For source=meta, URL obtained from llama.meta.com after accepting license terms\n",
            "  --max-parallel MAX_PARALLEL\n",
            "                        Maximum number of concurrent downloads\n",
            "  --ignore-patterns IGNORE_PATTERNS\n",
            "                        For source=huggingface, files matching any of the patterns are not downloaded. Defaults to ignoring\n",
            "                        safetensors files to avoid downloading duplicate weights.\n",
            "  --manifest-file MANIFEST_FILE\n",
            "                        For source=meta, you can download models from a manifest file containing a file => URL mapping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: download Llama3.2-1B-Instruct"
      ],
      "metadata": {
        "id": "WGXMGBHCWa-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INFERENCE_MODEL = \"Llama3.2-1B-Instruct\""
      ],
      "metadata": {
        "id": "xO9lvP7f2Wr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "llama model download \\\n",
        "--source huggingface \\\n",
        "--model-id \"Llama3.2-1B-Instruct\" \\\n",
        "--hf-token <<YOUR HUGGING FACE TOKEN>>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwUWzgKeEPgr",
        "outputId": "cc1ae6ea-9390-46f3-c6c2-6ab993d34d03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Successfully downloaded model to /root/.llama/checkpoints/Llama3.2-1B-Instruct\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]\rFetching 12 files:   8%|▊         | 1/12 [00:00<00:03,  3.24it/s]\rFetching 12 files:  58%|█████▊    | 7/12 [00:12<00:09,  1.86s/it]\rFetching 12 files: 100%|██████████| 12/12 [00:12<00:00,  1.05s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: download the run config"
      ],
      "metadata": {
        "id": "svqh9q58WjYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir -p distributions/meta-reference-gpu\n",
        "cd distributions/meta-reference-gpu\n",
        "wget https://raw.githubusercontent.com/jxtngx/llamastack-cookbook/refs/heads/main/python/run.yaml\n",
        "ls -l -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4n01kzCJAJH",
        "outputId": "1ab45acb-45a7-408f-9e8e-a457d7a5806b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 16\n",
            "drwxr-xr-x 3 root root 4096 Mar 16 00:23 .\n",
            "drwxr-xr-x 3 root root 4096 Mar 16 00:14 ..\n",
            "drwxr-xr-x 2 root root 4096 Mar 16 00:23 .ipynb_checkpoints\n",
            "-rw-r--r-- 1 root root 3725 Mar 16 00:23 run.yaml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "--2025-03-16 00:23:54--  https://raw.githubusercontent.com/jxtngx/llamastack-cookbook/refs/heads/main/python/run.yaml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3725 (3.6K) [text/plain]\n",
            "Saving to: ‘run.yaml’\n",
            "\n",
            "     0K ...                                                   100% 42.2M=0s\n",
            "\n",
            "2025-03-16 00:23:54 (42.2 MB/s) - ‘run.yaml’ saved [3725/3725]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: use Llama Stack as a library"
      ],
      "metadata": {
        "id": "8m6oQkRoWpbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "see: https://llama-stack.readthedocs.io/en/latest/distributions/importing_as_library.html#using-llama-stack-as-a-library"
      ],
      "metadata": {
        "id": "G5ng7ooCWu4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import the client"
      ],
      "metadata": {
        "id": "X9jnl1gtW1xB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_stack.distribution.library_client import LlamaStackAsLibraryClient"
      ],
      "metadata": {
        "id": "R2aaPRt8CIVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### initialize the client"
      ],
      "metadata": {
        "id": "Zqq4o-pRW5f-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLIENT_TEMPLATE = \"distributions/meta-reference-gpu/run.yaml\""
      ],
      "metadata": {
        "id": "A2708fCs25yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = LlamaStackAsLibraryClient(CLIENT_TEMPLATE)\n",
        "client.initialize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CzimHzlwCTkx",
        "outputId": "3ef02936-64d5-4b84-ce91-d120f127082e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: `bwrap` is not available. Code interpreter tool will not work correctly.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Using config \u001b[34mdistributions/meta-reference-gpu/run.yaml\u001b[0m:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using config <span style=\"color: #000080; text-decoration-color: #000080\">distributions/meta-reference-gpu/run.yaml</span>:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "apis:\n",
              "- agents\n",
              "- datasetio\n",
              "- eval\n",
              "- inference\n",
              "- safety\n",
              "- scoring\n",
              "- telemetry\n",
              "- tool_runtime\n",
              "- vector_io\n",
              "benchmarks: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
              "container_image: null\n",
              "datasets: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
              "image_name: meta-reference-gpu\n",
              "logging: null\n",
              "metadata_store:\n",
              "  db_path: \u001b[35m/root/.llama/distributions/meta-reference-gpu/\u001b[0m\u001b[95mregistry.db\u001b[0m\n",
              "  namespace: null\n",
              "  type: sqlite\n",
              "models:\n",
              "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "  model_id: Llama3.\u001b[1;36m2\u001b[0m-1B-Instruct\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - llm\n",
              "  provider_id: meta-reference-inference\n",
              "  provider_model_id: null\n",
              "- metadata:\n",
              "    embedding_dimension: \u001b[1;36m384\u001b[0m\n",
              "  model_id: all-MiniLM-L6-v2\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - embedding\n",
              "  provider_id: sentence-transformers\n",
              "  provider_model_id: null\n",
              "providers:\n",
              "  agents:\n",
              "  - config:\n",
              "      persistence_store:\n",
              "        db_path: \u001b[35m/root/.llama/distributions/meta-reference-gpu/\u001b[0m\u001b[95magents_store.db\u001b[0m\n",
              "        namespace: null\n",
              "        type: sqlite\n",
              "    provider_id: meta-reference\n",
              "    provider_type: inline::meta-reference\n",
              "  datasetio:\n",
              "  - config:\n",
              "      kvstore:\n",
              "        db_path: \u001b[35m/root/.llama/distributions/meta-reference-gpu/\u001b[0m\u001b[95mhuggingface_datasetio.db\u001b[0m\n",
              "        namespace: null\n",
              "        type: sqlite\n",
              "    provider_id: huggingface\n",
              "    provider_type: remote::huggingface\n",
              "  - config:\n",
              "      kvstore:\n",
              "        db_path: \u001b[35m/root/.llama/distributions/meta-reference-gpu/\u001b[0m\u001b[95mlocalfs_datasetio.db\u001b[0m\n",
              "        namespace: null\n",
              "        type: sqlite\n",
              "    provider_id: localfs\n",
              "    provider_type: inline::localfs\n",
              "  eval:\n",
              "  - config:\n",
              "      kvstore:\n",
              "        db_path: \u001b[35m/root/.llama/distributions/meta-reference-gpu/\u001b[0m\u001b[95mmeta_reference_eval.db\u001b[0m\n",
              "        namespace: null\n",
              "        type: sqlite\n",
              "    provider_id: meta-reference\n",
              "    provider_type: inline::meta-reference\n",
              "  inference:\n",
              "  - config:\n",
              "      checkpoint_dir: \u001b[32m'null'\u001b[0m\n",
              "      max_seq_len: \u001b[1;36m4096\u001b[0m\n",
              "      model: Llama3.\u001b[1;36m2\u001b[0m-1B-Instruct\n",
              "    provider_id: meta-reference-inference\n",
              "    provider_type: inline::meta-reference\n",
              "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "    provider_id: sentence-transformers\n",
              "    provider_type: inline::sentence-transformers\n",
              "  safety:\n",
              "  - config:\n",
              "      excluded_categories: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
              "    provider_id: llama-guard\n",
              "    provider_type: inline::llama-guard\n",
              "  scoring:\n",
              "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "    provider_id: basic\n",
              "    provider_type: inlin\u001b[1;92me::ba\u001b[0msic\n",
              "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "    provider_id: llm-as-judge\n",
              "    provider_type: inline::llm-as-judge\n",
              "  - config:\n",
              "      openai_api_key: \u001b[32m'********'\u001b[0m\n",
              "    provider_id: braintrust\n",
              "    provider_type: inlin\u001b[1;92me::b\u001b[0mraintrust\n",
              "  telemetry:\n",
              "  - config:\n",
              "      service_name: llama-stack\n",
              "      sinks: sqlite\n",
              "      sqlite_db_path: \u001b[35m/root/.llama/distributions/meta-reference-gpu/\u001b[0m\u001b[95mtrace_store.db\u001b[0m\n",
              "    provider_id: meta-reference\n",
              "    provider_type: inline::meta-reference\n",
              "  tool_runtime:\n",
              "  - config:\n",
              "      api_key: \u001b[32m'********'\u001b[0m\n",
              "      max_results: \u001b[1;36m3\u001b[0m\n",
              "    provider_id: brave-search\n",
              "    provider_type: remot\u001b[1;92me::b\u001b[0mrave-search\n",
              "  - config:\n",
              "      api_key: \u001b[32m'********'\u001b[0m\n",
              "      max_results: \u001b[1;36m3\u001b[0m\n",
              "    provider_id: tavily-search\n",
              "    provider_type: remote::tavily-search\n",
              "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "    provider_id: code-interpreter\n",
              "    provider_type: inlin\u001b[1;92me::c\u001b[0mode-interpreter\n",
              "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "    provider_id: rag-runtime\n",
              "    provider_type: inline::rag-runtime\n",
              "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "    provider_id: model-context-protocol\n",
              "    provider_type: remote::model-context-protocol\n",
              "  vector_io:\n",
              "  - config:\n",
              "      kvstore:\n",
              "        db_path: \u001b[35m/root/.llama/distributions/meta-reference-gpu/\u001b[0m\u001b[95mfaiss_store.db\u001b[0m\n",
              "        namespace: null\n",
              "        type: sqlite\n",
              "    provider_id: faiss\n",
              "    provider_type: inlin\u001b[1;92me::fa\u001b[0miss\n",
              "scoring_fns: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
              "server:\n",
              "  port: \u001b[1;36m8321\u001b[0m\n",
              "  tls_certfile: null\n",
              "  tls_keyfile: null\n",
              "shields: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
              "tool_groups:\n",
              "- args: null\n",
              "  mcp_endpoint: null\n",
              "  provider_id: tavily-search\n",
              "  toolgroup_id: builtin::websearch\n",
              "- args: null\n",
              "  mcp_endpoint: null\n",
              "  provider_id: rag-runtime\n",
              "  toolgroup_id: builtin::rag\n",
              "- args: null\n",
              "  mcp_endpoint: null\n",
              "  provider_id: code-interpreter\n",
              "  toolgroup_id: builtin::code_interpreter\n",
              "vector_dbs: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
              "version: \u001b[32m'2'\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">apis:\n",
              "- agents\n",
              "- datasetio\n",
              "- eval\n",
              "- inference\n",
              "- safety\n",
              "- scoring\n",
              "- telemetry\n",
              "- tool_runtime\n",
              "- vector_io\n",
              "benchmarks: <span style=\"font-weight: bold\">[]</span>\n",
              "container_image: null\n",
              "datasets: <span style=\"font-weight: bold\">[]</span>\n",
              "image_name: meta-reference-gpu\n",
              "logging: null\n",
              "metadata_store:\n",
              "  db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/root/.llama/distributions/meta-reference-gpu/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">registry.db</span>\n",
              "  namespace: null\n",
              "  type: sqlite\n",
              "models:\n",
              "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
              "  model_id: Llama3.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>-1B-Instruct\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - llm\n",
              "  provider_id: meta-reference-inference\n",
              "  provider_model_id: null\n",
              "- metadata:\n",
              "    embedding_dimension: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>\n",
              "  model_id: all-MiniLM-L6-v2\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - embedding\n",
              "  provider_id: sentence-transformers\n",
              "  provider_model_id: null\n",
              "providers:\n",
              "  agents:\n",
              "  - config:\n",
              "      persistence_store:\n",
              "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/root/.llama/distributions/meta-reference-gpu/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">agents_store.db</span>\n",
              "        namespace: null\n",
              "        type: sqlite\n",
              "    provider_id: meta-reference\n",
              "    provider_type: inline::meta-reference\n",
              "  datasetio:\n",
              "  - config:\n",
              "      kvstore:\n",
              "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/root/.llama/distributions/meta-reference-gpu/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">huggingface_datasetio.db</span>\n",
              "        namespace: null\n",
              "        type: sqlite\n",
              "    provider_id: huggingface\n",
              "    provider_type: remote::huggingface\n",
              "  - config:\n",
              "      kvstore:\n",
              "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/root/.llama/distributions/meta-reference-gpu/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">localfs_datasetio.db</span>\n",
              "        namespace: null\n",
              "        type: sqlite\n",
              "    provider_id: localfs\n",
              "    provider_type: inline::localfs\n",
              "  eval:\n",
              "  - config:\n",
              "      kvstore:\n",
              "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/root/.llama/distributions/meta-reference-gpu/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">meta_reference_eval.db</span>\n",
              "        namespace: null\n",
              "        type: sqlite\n",
              "    provider_id: meta-reference\n",
              "    provider_type: inline::meta-reference\n",
              "  inference:\n",
              "  - config:\n",
              "      checkpoint_dir: <span style=\"color: #008000; text-decoration-color: #008000\">'null'</span>\n",
              "      max_seq_len: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4096</span>\n",
              "      model: Llama3.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>-1B-Instruct\n",
              "    provider_id: meta-reference-inference\n",
              "    provider_type: inline::meta-reference\n",
              "  - config: <span style=\"font-weight: bold\">{}</span>\n",
              "    provider_id: sentence-transformers\n",
              "    provider_type: inline::sentence-transformers\n",
              "  safety:\n",
              "  - config:\n",
              "      excluded_categories: <span style=\"font-weight: bold\">[]</span>\n",
              "    provider_id: llama-guard\n",
              "    provider_type: inline::llama-guard\n",
              "  scoring:\n",
              "  - config: <span style=\"font-weight: bold\">{}</span>\n",
              "    provider_id: basic\n",
              "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::ba</span>sic\n",
              "  - config: <span style=\"font-weight: bold\">{}</span>\n",
              "    provider_id: llm-as-judge\n",
              "    provider_type: inline::llm-as-judge\n",
              "  - config:\n",
              "      openai_api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
              "    provider_id: braintrust\n",
              "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::b</span>raintrust\n",
              "  telemetry:\n",
              "  - config:\n",
              "      service_name: llama-stack\n",
              "      sinks: sqlite\n",
              "      sqlite_db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/root/.llama/distributions/meta-reference-gpu/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">trace_store.db</span>\n",
              "    provider_id: meta-reference\n",
              "    provider_type: inline::meta-reference\n",
              "  tool_runtime:\n",
              "  - config:\n",
              "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
              "      max_results: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
              "    provider_id: brave-search\n",
              "    provider_type: remot<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::b</span>rave-search\n",
              "  - config:\n",
              "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
              "      max_results: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
              "    provider_id: tavily-search\n",
              "    provider_type: remote::tavily-search\n",
              "  - config: <span style=\"font-weight: bold\">{}</span>\n",
              "    provider_id: code-interpreter\n",
              "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::c</span>ode-interpreter\n",
              "  - config: <span style=\"font-weight: bold\">{}</span>\n",
              "    provider_id: rag-runtime\n",
              "    provider_type: inline::rag-runtime\n",
              "  - config: <span style=\"font-weight: bold\">{}</span>\n",
              "    provider_id: model-context-protocol\n",
              "    provider_type: remote::model-context-protocol\n",
              "  vector_io:\n",
              "  - config:\n",
              "      kvstore:\n",
              "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/root/.llama/distributions/meta-reference-gpu/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">faiss_store.db</span>\n",
              "        namespace: null\n",
              "        type: sqlite\n",
              "    provider_id: faiss\n",
              "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::fa</span>iss\n",
              "scoring_fns: <span style=\"font-weight: bold\">[]</span>\n",
              "server:\n",
              "  port: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8321</span>\n",
              "  tls_certfile: null\n",
              "  tls_keyfile: null\n",
              "shields: <span style=\"font-weight: bold\">[]</span>\n",
              "tool_groups:\n",
              "- args: null\n",
              "  mcp_endpoint: null\n",
              "  provider_id: tavily-search\n",
              "  toolgroup_id: builtin::websearch\n",
              "- args: null\n",
              "  mcp_endpoint: null\n",
              "  provider_id: rag-runtime\n",
              "  toolgroup_id: builtin::rag\n",
              "- args: null\n",
              "  mcp_endpoint: null\n",
              "  provider_id: code-interpreter\n",
              "  toolgroup_id: builtin::code_interpreter\n",
              "vector_dbs: <span style=\"font-weight: bold\">[]</span>\n",
              "version: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### optional: test the client"
      ],
      "metadata": {
        "id": "X1TkiBssW9mF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.list()\n",
        "\n",
        "for model in response:\n",
        "    print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7PLynxwRQ9O",
        "outputId": "529bb1e6-40e7-45e1-bd16-af02e0160879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(identifier='Llama3.2-1B-Instruct', metadata={}, api_model_type='llm', provider_id='meta-reference-inference', provider_resource_id='Llama3.2-1B-Instruct', type='model', model_type='llm')\n",
            "Model(identifier='all-MiniLM-L6-v2', metadata={'embedding_dimension': 384.0}, api_model_type='embedding', provider_id='sentence-transformers', provider_resource_id='all-MiniLM-L6-v2', type='model', model_type='embedding')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### use chat completion"
      ],
      "metadata": {
        "id": "ZeyfPXBqXBuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.inference.chat_completion(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a friendly assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a two-sentence poem about llama.\"}\n",
        "    ],\n",
        "    model_id=INFERENCE_MODEL,\n",
        ")\n",
        "\n",
        "print(response.completion_message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jE09psXRp77",
        "outputId": "0abf09d9-c92c-43ff-970d-758790462e9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is a two-sentence poem about a llama:\n",
            "\n",
            "With soft and fuzzy fur so bright,\n",
            "The llama roams, a gentle sight.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your First RAG Agent"
      ],
      "metadata": {
        "id": "MgiK2_X-1ffE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "see: https://llama-stack.readthedocs.io/en/latest/getting_started/index.html#your-first-rag-agent"
      ],
      "metadata": {
        "id": "3IQOGBts1mMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import uuid\n",
        "from termcolor import cprint\n",
        "\n",
        "from llama_stack_client.lib.agents.agent import Agent\n",
        "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
        "from llama_stack_client.types import Document"
      ],
      "metadata": {
        "id": "2CyCRObj1wtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_library_client(template: str):\n",
        "    from llama_stack import LlamaStackAsLibraryClient\n",
        "\n",
        "    client = LlamaStackAsLibraryClient(template)\n",
        "    client.initialize()\n",
        "    return client"
      ],
      "metadata": {
        "id": "2mEafhfa1yEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = (create_library_client(CLIENT_TEMPLATE))  # or create_http_client() depending on the environment you picked"
      ],
      "metadata": {
        "id": "h9vN0Ma210Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Documents to be used for RAG\n",
        "urls = [\"chat.rst\", \"llama3.rst\", \"memory_optimizations.rst\", \"lora_finetune.rst\"]\n",
        "documents = [\n",
        "    Document(\n",
        "        document_id=f\"num-{i}\",\n",
        "        content=f\"https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/{url}\",\n",
        "        mime_type=\"text/plain\",\n",
        "        metadata={},\n",
        "    )\n",
        "    for i, url in enumerate(urls)\n",
        "]"
      ],
      "metadata": {
        "id": "oXtZBDJ111yQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_providers = [\n",
        "    provider for provider in client.providers.list() if provider.api == \"vector_io\"\n",
        "]\n",
        "provider_id = vector_providers[0].provider_id  # Use the first available vector provider"
      ],
      "metadata": {
        "id": "fi0bDz6h131c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register a vector database\n",
        "vector_db_id = f\"test-vector-db-{uuid.uuid4().hex}\"\n",
        "client.vector_dbs.register(\n",
        "    vector_db_id=vector_db_id,\n",
        "    provider_id=provider_id,\n",
        "    embedding_model=\"all-MiniLM-L6-v2\",\n",
        "    embedding_dimension=384,\n",
        ")"
      ],
      "metadata": {
        "id": "OTbrt1AG16jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert the documents into the vector database\n",
        "client.tool_runtime.rag_tool.insert(\n",
        "    documents=documents,\n",
        "    vector_db_id=vector_db_id,\n",
        "    chunk_size_in_tokens=512,\n",
        ")"
      ],
      "metadata": {
        "id": "atZ_bsEq17vU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_agent = Agent(\n",
        "    client,\n",
        "    model=INFERENCE_MODEL,\n",
        "    # Define instructions for the agent ( aka system prompt)\n",
        "    instructions=\"You are a helpful assistant\",\n",
        "    enable_session_persistence=False,\n",
        "    # Define tools available to the agent\n",
        "    tools=[\n",
        "        {\n",
        "            \"name\": \"builtin::rag/knowledge_search\",\n",
        "            \"args\": {\n",
        "                \"vector_db_ids\": [vector_db_id],\n",
        "            },\n",
        "        }\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "wId610lW19OR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session_id = rag_agent.create_session(\"test-session\")"
      ],
      "metadata": {
        "id": "uyLoe1vZ1_yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompts = [\n",
        "    \"How to optimize memory usage in torchtune? use the knowledge_search tool to get information.\",\n",
        "]"
      ],
      "metadata": {
        "id": "KrLRk-Ly2BKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the agent loop by calling the `create_turn` method\n",
        "for prompt in user_prompts:\n",
        "    cprint(f\"User> {prompt}\", \"green\")\n",
        "    response = rag_agent.create_turn(\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        session_id=session_id,\n",
        "    )\n",
        "    for log in EventLogger().log(response):\n",
        "        log.print()"
      ],
      "metadata": {
        "id": "obR9ubeN1e9d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}